{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ba9e3b",
   "metadata": {},
   "source": [
    "# Fake News Detection System using Deep Learning (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4721249",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "With the rapid growth of digital media, fake news spreads quickly through online platforms and social media, misleading people and creating social, political, and economic issues. Manually verifying news is time-consuming and inefficient. Therefore, there is a need for an automated system that can identify whether a news article is real or fake using machine learning and deep learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abee218",
   "metadata": {},
   "source": [
    "### Project Description / Information\n",
    "\n",
    "This project aims to build an intelligent fake news detection system using Natural Language Processing (NLP) and a Long Short-Term Memory (LSTM) deep learning model.\n",
    "The system processes news article text, converts it into numerical form using tokenization and padding, and then uses an LSTM neural network to learn sequential patterns in text. Based on the learned patterns, the model predicts whether the given news is Real or Fake.\n",
    "\n",
    "The model is trained on a labeled dataset containing real and fake news articles collected from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7514dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d554e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load Data\n",
    "df = pd.read_csv(r\"WELFake_Dataset.csv\")  # change path if needed\n",
    "df = df.dropna(subset=['text','label'])     # remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Prepare Text and Labels\n",
    "X = df['text']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Tokenize Text\n",
    "df = df.sample(15000, random_state=42)\n",
    "\n",
    "X = df['text'].astype(str).str.lower().str.replace('[^a-z ]', '', regex=True)\n",
    "y = df['label']\n",
    "\n",
    "max_words = 5000  # top most used words\n",
    "max_len = 100 # max length for each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3801eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "padded_data = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b2a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_data, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc841e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaishnavi\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\layers\\core\\embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c1d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 469ms/step - accuracy: 0.8204 - loss: 0.4457 - val_accuracy: 0.8825 - val_loss: 0.2869\n",
      "Epoch 2/5\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 459ms/step - accuracy: 0.9062 - loss: 0.2419 - val_accuracy: 0.8933 - val_loss: 0.2591\n",
      "Epoch 3/5\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 470ms/step - accuracy: 0.9349 - loss: 0.1804 - val_accuracy: 0.8804 - val_loss: 0.2815\n",
      "Epoch 4/5\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 465ms/step - accuracy: 0.9519 - loss: 0.1393 - val_accuracy: 0.8933 - val_loss: 0.2714\n",
      "Epoch 5/5\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 456ms/step - accuracy: 0.9600 - loss: 0.1171 - val_accuracy: 0.8975 - val_loss: 0.2836\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.8883 - loss: 0.3083\n",
      "Test Accuracy: 88.83%\n"
     ]
    }
   ],
   "source": [
    "#  Train Model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "#  Evaluate Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50d6a88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Prediction: [[0.75095737]]\n"
     ]
    }
   ],
   "source": [
    "real_news = \"\"\"\n",
    "India successfully launched a new weather observation satellite to improve\n",
    "cyclone forecasting and climate monitoring, according to officials from ISRO.\n",
    "The satellite will help provide accurate early warnings and reduce the impact\n",
    "of natural disasters.\n",
    "\"\"\"\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences([real_news])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "prediction = model.predict(padded_sequence)\n",
    "\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0852ee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " REAL NEWS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if prediction[0][0] > 0.5:\n",
    "    print(\" REAL NEWS\")\n",
    "else:\n",
    "    print(\" FAKE NEWS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff1b91",
   "metadata": {},
   "source": [
    "### Output / Results\n",
    "\n",
    "- The trained LSTM model successfully classifies news articles as Real or Fake\n",
    "\n",
    "- Achieved an accuracy of 88.97% on the test dataset\n",
    "\n",
    "- The system provides prediction confidence for each input news article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc072530",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "The Fake News Detection System effectively identifies fake and real news articles using deep learning techniques. The LSTM model performs well on news-style text data and demonstrates the practical application of NLP in real-world problems. This project highlights the potential of deep learning in combating misinformation and can be further enhanced by using domain-specific datasets and advanced models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
